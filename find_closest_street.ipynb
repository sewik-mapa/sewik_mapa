{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dcc5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, geopandas as gpd\n",
    "from rapidfuzz import process, fuzz\n",
    "from tqdm import tqdm\n",
    "from shapely import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dcb7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gps(lat_str, lon_str):\n",
    "    \"\"\"Parse GPS coordinates from Polish DMS format like \"21*22'806\" to decimal degrees\"\"\"\n",
    "    if pd.isna(lat_str) or lat_str == '' or pd.isna(lon_str) or lon_str == '':\n",
    "        return None\n",
    "    try:\n",
    "        # Try direct float conversion first (for decimal degrees)\n",
    "        return float(lat_str), float(lon_str)\n",
    "    except:\n",
    "        try:\n",
    "            # Parse degree*minute'second format like \"21*22'806\"\n",
    "            if '*' in str(lat_str) and \"'\" in str(lat_str) and '*' in str(lon_str) and \"'\" in str(lon_str):\n",
    "                lat_str, lon_str = str(lat_str).replace(',','').replace(\"''\",\"'\"), str(lon_str).replace(',','').replace(\"''\",\"'\")\n",
    "                parts_lat, parts_lon = lat_str.split('*'), lon_str.split('*')\n",
    "                min_sec_lat, min_sec_lon = parts_lat[1].split(\"'\"), parts_lon[1].split(\"'\")\n",
    "                minutes_lat, minutes_lon = float(min_sec_lat[0]), float(min_sec_lon[0])\n",
    "                seconds_lat, seconds_lon = float(min_sec_lat[0]), float(min_sec_lon[0])\n",
    "\n",
    "                if minutes_lat > 59 or minutes_lon > 59 or seconds_lat >= 600 or seconds_lon >= 600:\n",
    "                    return parse_as_decimal(lat_str), parse_as_decimal(lon_str)\n",
    "                else:\n",
    "                    return parse_as_degrees(lat_str), parse_as_degrees(lon_str)\n",
    "            else:\n",
    "                print(f\"Error parsing coordinate '{lat_str} {lon_str}': Invalid format\")  # Fixed undefined 'e'\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing coordinate '{lat_str} {lon_str}': {e}\")\n",
    "            return None\n",
    "\n",
    "def parse_as_decimal(coord_str):\n",
    "    try:\n",
    "        return float(coord_str)\n",
    "    except:\n",
    "        parts = coord_str.split('*', maxsplit=1)\n",
    "        ms = parts[1].split(\"'\", maxsplit=1)\n",
    "\n",
    "        try:\n",
    "            parts[0] = parts[0].replace(\"*\", \"0\")\n",
    "            ms[0] = ms[0].strip(\"*,`''\").replace(\"''\", \"'\").replace(\"*\", \"0\")\n",
    "            ms[1] = ms[1].strip(\"*,`''\").replace(\"''\", \"'\").replace(\"*\", \"0\")\n",
    "            ms[1] = 0 if ms[1] == '' else ms[1]\n",
    "            if len(ms[1]) <= 2:\n",
    "                div = 1\n",
    "            else:\n",
    "                div = 10\n",
    "            return float(parts[0]) + float(ms[0])/100 + (float(ms[1])/div)/10000\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing coordinate '{coord_str}': {e}\")\n",
    "            return None\n",
    "\n",
    "def parse_as_degrees(coord_str):\n",
    "    try:\n",
    "        return float(coord_str)\n",
    "    except:\n",
    "        parts = coord_str.split('*', maxsplit=1)\n",
    "        ms = parts[1].split(\"'\", maxsplit=1)\n",
    "        \n",
    "        try:\n",
    "            parts[0] = parts[0].replace(\"*\", \"0\")\n",
    "            ms[0] = ms[0].strip(\"*,`''\").replace(\"''\", \"'\").replace(\"*\", \"0\")\n",
    "            ms[1] = ms[1].strip(\"*,`''\").replace(\"''\", \"'\").replace(\"*\", \"0\")\n",
    "            ms[1] = 0 if ms[1] == '' else ms[1]\n",
    "\n",
    "            if len(ms[1]) <= 2:\n",
    "                div = 1\n",
    "            elif len(ms[1]) == 3:\n",
    "                div = 10\n",
    "            elif len(ms[1]) == 4:\n",
    "                div = 100\n",
    "            else:\n",
    "                div = 10\n",
    "\n",
    "            return float(parts[0]) + float(ms[0])/60 + (float(ms[1])/div)/3600\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing coordinate '{coord_str}': {e}\")\n",
    "            return None\n",
    "\n",
    "def best_match(street, choices):\n",
    "    \"\"\"Return best fuzzy match + score from a list of choices\"\"\"\n",
    "    if pd.isna(street) or not choices:\n",
    "        return None, 0\n",
    "    match, score, _ = process.extractOne(\n",
    "        street, choices, scorer=fuzz.token_sort_ratio\n",
    "    )\n",
    "    return match, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff63f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gminy = gpd.read_file(\"gis/A03_Granice_gmin.shp\", encoding='utf-8').to_crs(4326)\n",
    "powiaty = gpd.read_file(\"gis/A02_Granice_powiatow.shp\", encoding='utf-8').to_crs(4326)\n",
    "wojewodztwa = gpd.read_file(\"gis/A01_Granice_wojewodztw.shp\", encoding='utf-8').to_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af6ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    osm_streets = gpd.read_file(\"poland_pbf/poland-250914.osm.pbf\", layer=\"lines\")\n",
    "\n",
    "    osm_types = [\n",
    "        'residential', 'tertiary', 'pedestrian', 'secondary',\n",
    "        'living_street', 'primary', 'unclassified', 'service',\n",
    "        'secondary_link', 'trunk', 'construction', 'primary_link',\n",
    "        'motorway', 'tertiary_link', 'trunk_link', 'motorway_link']\n",
    "    osm_streets = osm_streets[osm_streets[\"highway\"].isin(osm_types)]\n",
    "\n",
    "    osm_streets = osm_selected_streets[[\"name\", \"highway\", \"geometry\"]].copy()\n",
    "    osm_streets = osm_streets.sjoin(gminy[[\"JPT_NAZWA_\", \"JPT_KOD_JE\", \"geometry\"]], how=\"left\")\n",
    "    osm_streets.drop(['index_right'], axis=1, errors=\"ignore\").to_file('gis/roads_poland.shp')\n",
    "else:\n",
    "    osm_streets = gpd.read_file('gis/roads_poland.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85bf15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_points = gpd.read_file(\"poland_pbf/poland-250914.osm.pbf\", layer=\"points\")\n",
    "osm_polygons = gpd.read_file(\"poland_pbf/poland-250914.osm.pbf\", layer=\"multipolygons\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b435715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_osm_attribute(df: pd.DataFrame, source_attr_name, target_attr_name):\n",
    "    df[f\"{source_attr_name}_loc\"] = df[\"other_tags\"].str.find(f'{source_attr_name}') + len(f'\"{source_attr_name}\"=>\"')\n",
    "    df[f\"{source_attr_name}_loc_end\"] = df.apply(lambda x: x[\"other_tags\"].find('\"', x[f'{source_attr_name}_loc']), axis=1)\n",
    "    df[f\"{target_attr_name}\"] = df.apply(lambda x: x[\"other_tags\"][x[f\"{source_attr_name}_loc\"]-1:x[f\"{source_attr_name}_loc_end\"]], axis=1)\n",
    "    return df\n",
    "\n",
    "if True:\n",
    "    # EXTRACT ADDRESSES FROM POLYGONS (BUILDINGS)\n",
    "    # osm_polygons = gpd.read_file(\"poland_pbf/poland-250914.osm.pbf\", layer=\"multipolygons\")\n",
    "    addresses_b = osm_polygons.loc[(~osm_polygons[\"other_tags\"].isna()) & (osm_polygons[\"other_tags\"].str.contains(\"addr\")), [\"geometry\", \"other_tags\"]]\n",
    "\n",
    "    addresses_b[\"geometry\"] = addresses_b.to_crs('+proj=cea').centroid.to_crs(addresses_b.crs)\n",
    "    addresses_b = extract_osm_attribute(addresses_b, 'addr:housenumber', 'addr_num')\n",
    "    addresses_b = extract_osm_attribute(addresses_b, 'addr:street', 'street_nm')\n",
    "    addresses_b = addresses_b[[\"addr_num\", \"street_nm\", \"geometry\"]]\n",
    "    addresses_b = addresses_b.sjoin(gminy[[\"JPT_NAZWA_\", \"JPT_KOD_JE\", \"geometry\"]], how=\"left\")\n",
    "\n",
    "    # EXTRACT ADDRESSES FROM POINTS\n",
    "    # osm_points = gpd.read_file(\"poland_pbf/poland-250914.osm.pbf\", layer=\"points\")\n",
    "\n",
    "    addresses_p = osm_points.loc[(~osm_points[\"other_tags\"].isna()) & (osm_points[\"other_tags\"].str.contains(\"addr\")), [\"geometry\", \"other_tags\"]]\n",
    "\n",
    "    addresses_p = extract_osm_attribute(addresses_p, 'addr:housenumber', 'addr_num')\n",
    "    addresses_p = extract_osm_attribute(addresses_p, 'addr:street', 'street_nm')\n",
    "    addresses_p = addresses_p[[\"addr_num\", \"street_nm\", \"geometry\"]]\n",
    "    addresses_p = addresses_p.sjoin(gminy[[\"JPT_NAZWA_\", \"JPT_KOD_JE\", \"geometry\"]], how=\"left\")\n",
    "\n",
    "    # MERGE AND SAVE\n",
    "    osm_adresses = pd.concat([addresses_b, addresses_p])\n",
    "    osm_adresses.drop(['index_right'], axis=1, errors=\"ignore\").to_file('gis/addr_poland.shp')\n",
    "else:\n",
    "    osm_adresses = gpd.read_file('gis/addr_poland.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df = pd.read_csv(\"csv/sewik_accidents.csv\")\n",
    "\n",
    "# Remove columns that start with \"Unnamed:\"\n",
    "accidents_df = accidents_df.loc[:, ~accidents_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "parse_gps_coords = True\n",
    "if parse_gps_coords:\n",
    "    coords = accidents_df.apply(lambda row: parse_gps(row['WSP_GPS_Y'], row['WSP_GPS_X']), axis=1)\n",
    "    \n",
    "    coord_list = coords.tolist()\n",
    "    accidents_df['lat'] = pd.Series([coord[0] if coord is not None else None for coord in coord_list])\n",
    "    accidents_df['lon'] = pd.Series([coord[1] if coord is not None else None for coord in coord_list])\n",
    "\n",
    "    # Convert to numeric, coercing errors to NaN\n",
    "    accidents_df['lon'] = pd.to_numeric(accidents_df['lon'], errors='coerce')\n",
    "    accidents_df['lat'] = pd.to_numeric(accidents_df['lat'], errors='coerce')\n",
    "\n",
    "accidents_df = gpd.GeoDataFrame(\n",
    "    accidents_df, \n",
    "    geometry=gpd.GeoSeries.from_xy(\n",
    "        accidents_df[\"lon\"], accidents_df[\"lat\"], crs=\"WGS84\")\n",
    ")\n",
    "if False:\n",
    "    # accidents_df.drop(\"geometry\", axis=1).to_csv(\"csv/sewik_accidents.csv\", index=False)\n",
    "\n",
    "    accidents_df.loc[\n",
    "        accidents_df.geometry.is_valid,\n",
    "            [\n",
    "                'ID', 'year', 'WOJ', 'WSP_GPS_X', 'WSP_GPS_Y', 'MIEJSCOWOSC' ,\n",
    "                'ULICA_ADRES', 'NUMER_DOMU', \"geometry\"]\n",
    "            ].to_file('gis/sewik_accidents_points.shp', driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da418d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "gminy[\"JPT_NAZWA_\"] = gminy[\"JPT_NAZWA_\"].astype(str).str.upper()\n",
    "powiaty[\"JPT_NAZWA_\"] = powiaty[\"JPT_NAZWA_\"].astype(str).str.upper()\n",
    "wojewodztwa[\"JPT_NAZWA_\"] = wojewodztwa[\"JPT_NAZWA_\"].astype(str).str.upper()\n",
    "\n",
    "# Debug: Print initial counts\n",
    "print(f\"Initial gminy count: {len(gminy)}\")\n",
    "print(f\"Initial powiaty count: {len(powiaty)}\")\n",
    "print(f\"Initial wojewodztwa count: {len(wojewodztwa)}\")\n",
    "\n",
    "gminy[\"JPT_KOD_JE\"] = gminy[\"JPT_KOD_JE\"].astype(str)\n",
    "gminy[\"KOD_POW\"] = gminy[\"JPT_KOD_JE\"].astype(str).str[:4]\n",
    "gminy[\"KOD_WOJ\"] = gminy[\"JPT_KOD_JE\"].astype(str).str[:2]\n",
    "powiaty[\"KOD_WOJ\"] = powiaty[\"JPT_KOD_JE\"].astype(str).str[:2]\n",
    "powiaty[\"KOD_POW\"] = powiaty[\"JPT_KOD_JE\"].astype(str)\n",
    "\n",
    "powiaty[\"JPT_KOD_JE\"] = powiaty[\"JPT_KOD_JE\"]\n",
    "wojewodztwa[\"JPT_KOD_JE\"] = wojewodztwa[\"JPT_KOD_JE\"]\n",
    "\n",
    "# Debug: Check for missing KOD_POW matches before merge\n",
    "gminy_kod_pow = set(gminy[\"KOD_POW\"].unique())\n",
    "powiaty_kod_pow = set(powiaty[\"KOD_POW\"].unique())\n",
    "missing_pow_codes = gminy_kod_pow - powiaty_kod_pow\n",
    "print(f\"Unique KOD_POW in gminy: {len(gminy_kod_pow)}\")\n",
    "print(f\"Unique KOD_POW in powiaty: {len(powiaty_kod_pow)}\")\n",
    "print(f\"Missing KOD_POW codes in powiaty: {len(missing_pow_codes)}\")\n",
    "if missing_pow_codes:\n",
    "    print(f\"Missing codes sample: {list(missing_pow_codes)[:10]}\")\n",
    "\n",
    "# Use outer join to preserve all gminas and identify missing matches\n",
    "gminy_powiaty = gminy[[\"JPT_KOD_JE\", \"JPT_NAZWA_\", \"KOD_POW\"]].rename({\"JPT_KOD_JE\": \"KOD_GMI\", \"JPT_NAZWA_\": \"GMINA\"}, axis=1).merge(\n",
    "    powiaty[[\"JPT_KOD_JE\", \"JPT_NAZWA_\", \"KOD_WOJ\"]].rename({\"JPT_NAZWA_\": \"POWIAT\", \"JPT_KOD_JE\": \"KOD_POW\"}, axis=1),\n",
    "    on=\"KOD_POW\", how=\"left\")\n",
    "\n",
    "print(f\"After first merge: {len(gminy_powiaty)}\")\n",
    "print(f\"Records with missing POWIAT: {len(gminy_powiaty[gminy_powiaty['POWIAT'].isna()])}\")\n",
    "\n",
    "# Check for missing KOD_WOJ matches before second merge\n",
    "gminy_kod_woj = set(gminy_powiaty[\"KOD_WOJ\"].dropna().unique())\n",
    "wojewodztwa_kod_woj = set(wojewodztwa[\"JPT_KOD_JE\"].unique())\n",
    "missing_woj_codes = gminy_kod_woj - wojewodztwa_kod_woj\n",
    "print(f\"Missing KOD_WOJ codes in wojewodztwa: {len(missing_woj_codes)}\")\n",
    "\n",
    "gminy_powiaty = gminy_powiaty.merge(\n",
    "    wojewodztwa[[\"JPT_KOD_JE\", \"JPT_NAZWA_\"]].rename({\"JPT_NAZWA_\": \"WOJ\", \"JPT_KOD_JE\": \"KOD_WOJ\"}, axis=1),\n",
    "    on=\"KOD_WOJ\", how=\"left\")\n",
    "\n",
    "print(f\"After second merge: {len(gminy_powiaty)}\")\n",
    "print(f\"Records with missing WOJ: {len(gminy_powiaty[gminy_powiaty['WOJ'].isna()])}\")\n",
    "\n",
    "# Show samples of missing data for debugging\n",
    "if len(gminy_powiaty[gminy_powiaty[\"POWIAT\"].isna()]) > 0:\n",
    "    print(\"\\nSample gminas without matching powiat:\")\n",
    "    print(gminy_powiaty[gminy_powiaty[\"POWIAT\"].isna()][[\"GMINA\", \"KOD_POW\"]].head())\n",
    "\n",
    "# Modified condition to warn instead of raise exception\n",
    "missing_powiaty = gminy_powiaty[gminy_powiaty[\"POWIAT\"].isna()]\n",
    "if len(missing_powiaty) > 0:\n",
    "    print(f\"WARNING: {len(missing_powiaty)} gminas could not be matched to powiaty\")\n",
    "    # You can choose to either filter them out or keep them with missing data\n",
    "    # gminy_powiaty = gminy_powiaty.dropna(subset=[\"POWIAT\"])  # Uncomment to remove unmatched records\n",
    "\n",
    "powiaty_for_join = gminy_powiaty[~gminy_powiaty[[\"KOD_POW\"]].duplicated(keep='first')][[\"KOD_POW\", \"POWIAT\", \"KOD_WOJ\", \"WOJ\"]]\n",
    "print(f\"Final powiaty_for_join count: {len(powiaty_for_join)}\")\n",
    "\n",
    "gminy_obwarzanki = gminy_powiaty[gminy_powiaty[[\"WOJ\", \"POWIAT\", \"GMINA\"]].duplicated(keep=False)]\n",
    "gminy_obwarzanki.loc[gminy_obwarzanki[\"KOD_GMI\"].str[-1]== '1', \"GMINA\"] = gminy_obwarzanki.loc[gminy_obwarzanki[\"KOD_GMI\"].str[-1]== '1', \"GMINA\"] + \" - OBSZAR WIEJSKI\"\n",
    "gminy_obwarzanki.loc[gminy_obwarzanki[\"KOD_GMI\"].str[-1]== '2', \"GMINA\"] = gminy_obwarzanki.loc[gminy_obwarzanki[\"KOD_GMI\"].str[-1]== '2', \"GMINA\"] + \" - OBSZAR MIEJSKI\"\n",
    "\n",
    "gminy_powiaty = pd.concat([gminy_powiaty[~gminy_powiaty[[\"WOJ\", \"POWIAT\", \"GMINA\"]].duplicated(keep=False)], gminy_obwarzanki])\n",
    "print(f\"Final gminy_powiaty count: {len(gminy_powiaty)}\")\n",
    "\n",
    "miasta_na_prawach_powiatu = gminy_powiaty.copy()\n",
    "miasta_na_prawach_powiatu[\"GMINA\"] = \"POWIAT \" + miasta_na_prawach_powiatu[\"GMINA\"]\n",
    "miasta_na_prawach_powiatu = miasta_na_prawach_powiatu[miasta_na_prawach_powiatu[\"POWIAT\"]==miasta_na_prawach_powiatu[\"GMINA\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1ef0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# czyszczenie gmin\n",
    "accident_gmina_correspondence = {\n",
    "    'WARSZAWA BIAŁOŁĘKA': 'WARSZAWA',\n",
    "    'WARSZAWA MOKOTÓW': 'WARSZAWA',\n",
    "    'OSTROWICE': 'DRAWSKO POMORSKIE', # zlikwidowana w 2018 z powodu niewyplacalnosci\n",
    "    'SITKÓWKA-NOWINY': 'NOWINY', # zmiana nazwy\n",
    "    'JEDLNIA - LETNISKO': 'JEDLNIA-LETNISKO'\n",
    "}\n",
    "\n",
    "\n",
    "if \"GMINA_org\" not in accidents_df.columns:\n",
    "    for old, new in accident_gmina_correspondence.items():\n",
    "        accidents_df[\"GMINA_org\"] = accidents_df[\"GMINA\"].str.replace(old, new)\n",
    "else:\n",
    "    print (\"Column GMINA_org already existing\")\n",
    "    for old, new in accident_gmina_correspondence.items():\n",
    "        accidents_df[\"GMINA_org\"] = accidents_df[\"GMINA_org\"].str.replace(old, new)\n",
    "\n",
    "# gminas_to_correct = accidents_df[[\"GMINA_org\", \"GMINA\"]].drop_duplicates()\n",
    "# gminas_to_correct = gminas_to_correct[~gminas_to_correct[\"GMINA\"].duplicated(keep=False)]\n",
    "\n",
    "accidents_df.loc[~accidents_df[\"GMINA_org\"].isin(gminy_obwarzanki[\"GMINA\"]), \"GMINA\"] = (\n",
    "    accidents_df.loc[~accidents_df[\"GMINA_org\"].isin(gminy_obwarzanki[\"GMINA\"]), \"GMINA_org\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\" - OBSZAR WIEJSKI\", \"\", regex=False)\n",
    "    .str.replace(\" - OBSZAR MIEJSKI\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "# accidents_df[(accidents_df[\"GMINA\"].str.contains(\"KARPACZ\")) & (~accidents_df[\"GMINA_org\"].isin(gminy_obwarzanki[\"GMINA\"]))][\"GMINA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21794cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accidents_df[(accidents_df[\"GMINA\"].str.contains(\"KARPACZ\")) & (~accidents_df[\"GMINA_org\"].isin(gminy_obwarzanki[\"GMINA\"]))][\"GMINA\"]\n",
    "acc_join = accidents_df.merge(gminy_powiaty, on=[\"WOJ\", \"POWIAT\", \"GMINA\"], how=\"left\")\n",
    "\n",
    "acc_join_mnp = acc_join.loc[~acc_join[\"GMINA\"].isin(gminy_powiaty[\"GMINA\"])].drop(['KOD_POW', 'KOD_WOJ', 'GMINA', 'KOD_GMI'], axis=1).merge(miasta_na_prawach_powiatu, on=[\"WOJ\", \"POWIAT\"], how=\"left\")\n",
    "acc_join_mnp[\"GMINA\"] = acc_join_mnp[\"GMINA\"].str.replace('POWIAT ', '')\n",
    "\n",
    "acc_join_mnp_remaining = acc_join_mnp[~acc_join_mnp[\"GMINA\"].isna()].copy()\n",
    "acc_join_mnp_remaining[\"GMINA\"] = acc_join_mnp_remaining[\"MIEJSCOWOSC\"]\n",
    "acc_join_mnp_remaining = acc_join_mnp_remaining.drop(['KOD_POW', 'KOD_WOJ', 'POWIAT', 'KOD_GMI'], axis=1).merge(gminy_powiaty, on=[\"WOJ\", \"GMINA\"], how=\"left\")\n",
    "\n",
    "acc_join_mnp = acc_join_mnp[acc_join_mnp[\"GMINA\"].isna()]\n",
    "\n",
    "acc_join_mnp_remaining = pd.concat([\n",
    "        acc_join_mnp_remaining[~(acc_join_mnp_remaining[\"ID\"].duplicated(keep=False))],\n",
    "        acc_join_mnp_remaining[(acc_join_mnp_remaining[\"ID\"].duplicated(keep=\"last\"))]\n",
    "    ])\n",
    "\n",
    "new_accidents_df = pd.concat([\n",
    "        acc_join.loc[acc_join[\"GMINA\"].isin(gminy_powiaty[\"GMINA\"])],\n",
    "        acc_join_mnp, acc_join_mnp_remaining\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df_sjoined = new_accidents_df.sjoin(gminy[[\"JPT_NAZWA_\", \"JPT_KOD_JE\", \"geometry\"]], how=\"left\")\n",
    "name_matches = accidents_df_sjoined[\"JPT_NAZWA_\"].str.upper().isin(\n",
    "    accidents_df_sjoined[\"MIEJSCOWOSC\"].str.upper())\n",
    "accidents_df_sjoined[~name_matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0159e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = new_accidents_df[~new_accidents_df[\"ID\"].isin(accidents_df_sjoined[\"ID\"])]\n",
    "if (len(check) > 0) or (len(new_accidents_df) != len(accidents_df_sjoined)) or len(new_accidents_df) != len(accidents_df):\n",
    "    raise Exception(\"Some accidents were lost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d840772",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df_sjoined[\"MIEJSCOWOSC\"] = accidents_df_sjoined[\"MIEJSCOWOSC\"].astype(str).str.upper()\n",
    "osm_streets[\"JPT_NAZWA_\"] = osm_streets[\"JPT_NAZWA_\"].astype(str).str.upper()\n",
    "osm_streets[\"name\"] = osm_streets[\"name\"].astype(str).str.upper()\n",
    "osm_streets = osm_streets[osm_streets[\"name\"]!='NONE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe77d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df_sjoined[\"MIEJSCOWOSC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf634b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "\n",
    "# Ensure both dataframes have the JPT_NAZWA_ column in uppercase for matching\n",
    "accidents_df_sjoined[\"JPT_NAZWA_\"] = accidents_df_sjoined[\"JPT_NAZWA_\"].str.upper()\n",
    "osm_streets[\"JPT_NAZWA_\"] = osm_streets[\"JPT_NAZWA_\"].str.upper()\n",
    "\n",
    "# Get unique municipalities from accidents data\n",
    "\n",
    "accidents_to_correct = accidents_df_sjoined[\n",
    "    ((accidents_df_sjoined[\"MIEJSCOWOSC\"] == \"WARSZAWA\") | (accidents_df_sjoined[\"JPT_KOD_JE\"] != accidents_df_sjoined[\"KOD_GMI\"]))]\n",
    "\n",
    "gminas_to_check = accidents_to_correct[\"KOD_GMI\"].dropna().unique()\n",
    "\n",
    "# Calculate total number of accidents to process\n",
    "total_accidents = len(accidents_to_correct)\n",
    "\n",
    "# Initialize progress bar with ETA\n",
    "pbar = tqdm(total=total_accidents, desc=\"Processing accidents\")\n",
    "\n",
    "total_matches = 0\n",
    "processed_count = 0\n",
    "last_update = 0\n",
    "\n",
    "# Process each municipality separately\n",
    "for kod_gminy in gminas_to_check:\n",
    "    # Get accidents in this municipality\n",
    "    gmina = gminy_powiaty.loc[gminy_powiaty[\"KOD_GMI\"] == kod_gminy, \"GMINA\"].iloc[0]\n",
    "    accidents_in_municipality = accidents_to_correct[\n",
    "        (accidents_to_correct[\"KOD_GMI\"] == kod_gminy) &\n",
    "        ((accidents_to_correct[\"GMINA\"] == \"WARSZAWA\") | (accidents_to_correct[\"JPT_KOD_JE\"] != accidents_to_correct[\"KOD_GMI\"]))\n",
    "    ]\n",
    "    \n",
    "    # Get streets in this municipality\n",
    "    streets_in_municipality = osm_streets[osm_streets[\"JPT_KOD_JE\"] == kod_gminy]\n",
    "    adresses_in_municipality = osm_adresses[osm_adresses[\"JPT_KOD_JE\"] == kod_gminy]\n",
    "    \n",
    "    if streets_in_municipality.empty:\n",
    "        # Update progress bar for accidents we can't process\n",
    "        pbar.update(len(accidents_in_municipality))\n",
    "        processed_count += len(accidents_in_municipality)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Get unique street names for fuzzy matching (keep original case)\n",
    "    street_names = streets_in_municipality[\"name\"].dropna().unique().tolist()\n",
    "\n",
    "    if not street_names:\n",
    "        # Update progress bar for accidents we can't process\n",
    "        pbar.update(len(accidents_in_municipality))\n",
    "        processed_count += len(accidents_in_municipality)\n",
    "        continue\n",
    "    \n",
    "    # Match each accident to the best street\n",
    "    for idx, accident in accidents_in_municipality.iterrows():\n",
    "        accident_street = accident[\"ULICA_ADRES\"]\n",
    "        accident_address = accident[\"NUMER_DOMU\"]\n",
    "        processed_count += 1\n",
    "        \n",
    "        if pd.isna(accident_street) or accident_street == '':\n",
    "            # For missing street names, keep current coordinates and create a basic record\n",
    "            if processed_count - last_update >= 50:\n",
    "                pbar.update(processed_count - last_update)\n",
    "                match_pct = (total_matches / processed_count * 100) if processed_count > 0 else 0\n",
    "                pbar.set_postfix({\n",
    "                    'matches': total_matches,\n",
    "                    'match_rate': f'{match_pct:.1f}%'\n",
    "                })\n",
    "                last_update = processed_count\n",
    "            \n",
    "            # Create record with no street match but keep original coordinates\n",
    "            match_record = {\n",
    "                \"accident_id\": accident[\"ID\"],\n",
    "                \"municipality\": gmina,\n",
    "                \"accident_street\": None,\n",
    "                \"matched_street\": None,\n",
    "                \"similarity_score\": None,\n",
    "                \"distance_to_street\": None,\n",
    "                \"lat\": accident[\"lat\"],\n",
    "                \"lon\": accident[\"lon\"],\n",
    "                \"WSP_GPS_X\": accident[\"WSP_GPS_X\"],\n",
    "                \"WSP_GPS_Y\": accident[\"WSP_GPS_Y\"],\n",
    "                \"accident_geometry\": accident[\"geometry\"],\n",
    "                \"street_geometry\": None\n",
    "            }\n",
    "            matches.append(match_record)\n",
    "            total_matches += 1\n",
    "            continue\n",
    "\n",
    "        # Find best matching street name (case-insensitive)\n",
    "        best_street, similarity_score = best_match(accident_street, street_names)\n",
    "\n",
    "        # Lower minimum threshold but add better validation\n",
    "        if best_street and similarity_score >= 60:  # Reduced from 75 to 60            \n",
    "            # Get ALL street segments with this name (case-insensitive search)\n",
    "            matched_streets = streets_in_municipality[\n",
    "                streets_in_municipality[\"name\"].str.upper() == str(best_street).upper()]\n",
    "\n",
    "            # Find the closest street segment geometrically\n",
    "            # Find the closest street segment geometrically\n",
    "            accident_point = accident[\"geometry\"]\n",
    "            best_distance = float('inf')\n",
    "            closest_street = None\n",
    "            best_coords = None\n",
    "            \n",
    "            # First, let's parse coordinates once for this accident\n",
    "            coord_y_clean = str(accident[\"WSP_GPS_Y\"]).replace(\"''\",\"'\").replace(',','')\n",
    "            coord_x_clean = str(accident[\"WSP_GPS_X\"]).replace(\"''\",\"'\").replace(',','')\n",
    "            \n",
    "            decimal_lat = parse_as_decimal(coord_y_clean)\n",
    "            decimal_lon = parse_as_decimal(coord_x_clean)\n",
    "            degree_lat = parse_as_degrees(coord_y_clean)\n",
    "            degree_lon = parse_as_degrees(coord_x_clean)\n",
    "            \n",
    "            # Test which coordinate parsing gives better results\n",
    "            candidate_points = []\n",
    "            if decimal_lat is not None and decimal_lon is not None:\n",
    "                candidate_points.append({\"point\": Point(decimal_lon, decimal_lat), \"lat\": decimal_lat, \"lon\": decimal_lon, \"type\": \"decimal\"})\n",
    "            if degree_lat is not None and degree_lon is not None:\n",
    "                candidate_points.append({\"point\": Point(degree_lon, degree_lat), \"lat\": degree_lat, \"lon\": degree_lon, \"type\": \"degree\"})\n",
    "            if decimal_lat is not None and degree_lon is not None:\n",
    "                candidate_points.append({\"point\": Point(degree_lon, decimal_lat), \"lat\": decimal_lat, \"lon\": degree_lon, \"type\": \"decimal_lat_degree_lon\"})\n",
    "            if degree_lat is not None and decimal_lon is not None:\n",
    "                candidate_points.append({\"point\": Point(decimal_lon, degree_lat), \"lat\": degree_lat, \"lon\": decimal_lon, \"type\": \"degree_lat_decimal_lon\"})\n",
    "\n",
    "            # Add original coordinates if available\n",
    "            if not pd.isna(accident[\"lat\"]) and not pd.isna(accident[\"lon\"]):\n",
    "                candidate_points.append({\"point\": Point(accident[\"lon\"], accident[\"lat\"]), \"lat\": accident[\"lat\"], \"lon\": accident[\"lon\"], \"type\": \"original\"})\n",
    "            \n",
    "            for _, street_segment in matched_streets.iterrows():\n",
    "                # Test all coordinate interpretations against this street segment\n",
    "                for coord_candidate in candidate_points:\n",
    "                    distance = coord_candidate[\"point\"].distance(street_segment[\"geometry\"])\n",
    "                    \n",
    "                    if distance < best_distance:\n",
    "                        best_distance = distance\n",
    "                        best_coords = coord_candidate\n",
    "                        closest_street = street_segment\n",
    "            \n",
    "            # Convert distance to meters for validation (approximate for Poland ~52°N)\n",
    "            distance_meters = best_distance * 111320\n",
    "            \n",
    "            # Only accept matches if they're within reasonable distance (e.g., 500 meters)\n",
    "            # AND have good similarity score, OR very close distance with moderate similarity\n",
    "            is_valid_match = False\n",
    "            \n",
    "            if distance_meters <= 100 and similarity_score >= 60:  # Very close, moderate similarity OK\n",
    "                is_valid_match = True\n",
    "            elif distance_meters <= 200 and similarity_score >= 70:  # Close, good similarity\n",
    "                is_valid_match = True\n",
    "            elif distance_meters <= 500 and similarity_score >= 80:  # Moderate distance, high similarity\n",
    "                is_valid_match = True\n",
    "            elif similarity_score > 95:  # Almost same\n",
    "                is_valid_match = True\n",
    "            match_record = {\n",
    "                \"accident_id\": accident[\"ID\"],\n",
    "                \"municipality\": gmina,\n",
    "                \"accident_street\": accident_street,\n",
    "                \"matched_street\": best_street,\n",
    "                \"similarity_score\": similarity_score,\n",
    "                \"distance_to_street\": best_distance,\n",
    "                \"distance_to_street_meters\": distance_meters,\n",
    "                \"WSP_GPS_X\": accident[\"WSP_GPS_X\"],\n",
    "                \"WSP_GPS_Y\": accident[\"WSP_GPS_Y\"],\n",
    "                \"best_lat\": None,\n",
    "                \"best_lon\": None,\n",
    "                \"best_coord_type\": None,\n",
    "                \"lat\": accident[\"lat\"],\n",
    "                \"lon\": accident[\"lon\"],\n",
    "                \"accident_geometry\": accident[\"geometry\"],\n",
    "                \"street_geometry\": None\n",
    "            }\n",
    "\n",
    "            if is_valid_match and closest_street is not None:\n",
    "                match_record[\"best_lat\"] = best_coords[\"lat\"]\n",
    "                match_record[\"best_lon\"] = best_coords[\"lon\"]\n",
    "                match_record[\"best_coord_type\"] = best_coords[\"type\"]\n",
    "                match_record[\"street_geometry\"] = closest_street[\"geometry\"]\n",
    "                total_matches += 1\n",
    "                \n",
    "            matches.append(match_record)\n",
    "\n",
    "        # Only update progress bar every 50 records\n",
    "        if processed_count - last_update >= 50:\n",
    "            pbar.update(processed_count - last_update)\n",
    "            match_pct = (total_matches / processed_count * 100) if processed_count > 0 else 0\n",
    "            pbar.set_postfix({\n",
    "                'matches': total_matches,\n",
    "                'match_rate': f'{match_pct:.1f}%'\n",
    "            })\n",
    "            last_update = processed_count\n",
    "\n",
    "# Final update for any remaining records\n",
    "if processed_count > last_update:\n",
    "    pbar.update(processed_count - last_update)\n",
    "    match_pct = (total_matches / processed_count * 100) if processed_count > 0 else 0\n",
    "    pbar.set_postfix({\n",
    "        'matches': total_matches,\n",
    "        'match_rate': f'{match_pct:.1f}%'\n",
    "    })\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Create final dataframe\n",
    "final_df = pd.DataFrame(matches)\n",
    "\n",
    "# Convert distance from degrees to meters (approximate conversion at Poland's latitude)\n",
    "# 1 degree ≈ 111,320 meters at the equator, but we need to account for latitude\n",
    "# For Poland (around 52°N), 1 degree longitude ≈ 69,470 meters, 1 degree latitude ≈ 111,320 meters\n",
    "# We'll use an average approximation for the distance calculation\n",
    "if len(final_df) > 0:\n",
    "    # Convert distance from degrees to meters (approximate conversion for Poland's latitude ~52°N)\n",
    "    # Using Haversine approximation: 1 degree ≈ 111,320 meters\n",
    "    final_df['distance_to_street_meters'] = final_df['distance_to_street'] * 111320\n",
    "    \n",
    "    # Round to 1 decimal place for readability\n",
    "    final_df['distance_to_street_meters'] = final_df['distance_to_street_meters'].round(1)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"FINAL RESULTS:\")\n",
    "print(f\"Total matches created: {len(final_df)}\")\n",
    "if len(final_df) > 0:\n",
    "    print(f\"Similarity score distribution:\")\n",
    "    print(final_df[\"similarity_score\"].describe())\n",
    "    \n",
    "    print(f\"\\nDistance to street distribution (meters):\")\n",
    "    print(final_df[\"distance_to_street_meters\"].describe())\n",
    "    \n",
    "    # Filter for high-quality matches\n",
    "    high_quality_matches = final_df[final_df[\"similarity_score\"] >= 80]\n",
    "    print(f\"High quality matches (>= 80% similarity): {len(high_quality_matches)}\")\n",
    "    print(f\"Medium quality matches (70-79% similarity): {len(final_df) - len(high_quality_matches)}\")\n",
    "    \n",
    "    # Show closest matches\n",
    "    closest_matches = final_df.nsmallest(10, 'distance_to_street_meters')\n",
    "    print(f\"\\n10 closest matches:\")\n",
    "    print(closest_matches[['accident_id', 'accident_street', 'matched_street', 'similarity_score', 'distance_to_street_meters']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b2c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(matches)\n",
    "\n",
    "final_df.to_csv(\"gis/street_matches_v3.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24b3a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_latlon = final_df[[\"accident_id\", \"best_lat\", \"best_lon\"]].rename({\"accident_id\": \"ID\", \"best_lat\": \"lat\", \"best_lon\": \"lon\"}, axis=1).dropna().set_index(\"ID\")\n",
    "updated_latlon = updated_latlon[~updated_latlon.index.duplicated()]\n",
    "accidents_df = accidents_df.set_index(\"ID\")\n",
    "accidents_df.update(updated_latlon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df.reset_index().drop(\"geometry\", axis=1).to_csv(\"csv/sewik_accidents_v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81c90c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df = gpd.GeoDataFrame(\n",
    "    accidents_df, \n",
    "    geometry=gpd.GeoSeries.from_xy(\n",
    "        accidents_df[\"lon\"], accidents_df[\"lat\"], crs=\"WGS84\"))\n",
    "\n",
    "accidents_df.reset_index().loc[\n",
    "    accidents_df.reset_index().geometry.is_valid,\n",
    "        [\n",
    "            'ID', 'year', 'WOJ', 'WSP_GPS_X', 'WSP_GPS_Y', 'MIEJSCOWOSC' ,\n",
    "            'ULICA_ADRES', 'NUMER_DOMU', \"geometry\"]\n",
    "        ].to_file('gis/sewik_accidents_points_v3.shp', driver='ESRI Shapefile')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
